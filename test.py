import torch
from transformers import AutoTokenizer, BartForConditionalGeneration

tokenizer = AutoTokenizer.from_pretrained('kobart-summarization')
model = BartForConditionalGeneration.from_pretrained('kobart-summarization')

text = "산업화에 따라 사회가 분화되고 개인이 공동체적 유대로부터 벗어나게 되는 현상을 ‘개체화’라고 한다. 울리히 벡과 지그문트 바우만은 현대의 개체화 현상, 사회적 위험 문제와 연관지어 진단한 대표적인 학자들이다. 사실 사회 분화와 개체화는 자본주의적 산업화 이래로 지속된 현상이다. 그런데 20세기 중반 이후부터는 세계화를 계기로 개체화 현상이 본격화되는 경향으로 더욱 양상을 달리 보이고 있다. 벡은 분화 과정이란 고도로 따라 각종 위험을 낳는다는 것과 노동의 종말이 가속화되면서 개인에 대한 책임이 더욱 강조된다고 본다. 벡은 이러한 진단 위에서 세계화 시대에 불확실성과 불안정이 증대됨에 따라 새로운 위험에 노출된 개인들이 생존을 위협받고 있으며, 이에 따라 자신들의 삶을 사회적으로 보장받을 수 있는 조건이 약화되고 있다고 본다. 특히, 전통적인 가족이 해체되고 개인이 가족이나 공동체로부터 분리되어 살아가는 일이 가구가 급속도로 늘어나는 등 가족의 해체 현상도 많이 나타나고 있다. 벡과 바우만은 개체화의 이러한 가족화 추세에 대해서 인식의 차이를 보이긴 한다. 그런데 현대의 위기와 관련해서 그들이 개체화를 바라보는 시각은 사뭇 다르다. 벡은 과학 기술의 의도하지 않은 결과들로 나타난 위험이 단순히 자연재해나 파괴로 귀결되는 것이 아니라, 벡은 무규칙의 불안 속 사고, 질병, 실업 등 예측 불가능함을 위협이 현실화된 상황이 있다는 데서 삶의 편의와 풍요를 위해 이용한 방식(과학기술)으로써 위협이 세계화되도록 하고 있다고 보았다. 그는 위험이 현대의 개체화된 개인을 더욱 고립시키고 불안하게 만드는 조건으로 작용하고 있으며, 개개인과 국가가 서로를 거리낌 없이 비난하고 책임을 전가하는 것이 벡이 진단한 현대 사회의 모습이다. 반면, 현대인들이 개체화되었다는 바로 그 조건 때문에 오히려 그들은 사회적 관계를 갈망하고 공동체적 유대와 초국가적으로 연대할 가능성이 있다고 보았다. 특히 벡은 그들이 피한 기술의 발견뿐 아니라 그 과정과 결과까지 인식함과 더불어 모색하는 성찰적 근대화의 실천 주체로서의 인식론적 대안을 요구를 모아 정리하였다. 고흐(盧曄) 학자는 공동체의 나아갈 단서로 주목받았다. 한편 바우만은 개체화된 ‘고립된 삶의 불확실성’ 속에서 생존을 모색하게 된 현대를 ‘액체 시대’로 정의하였다. 현대인의 삶은 유동적이며 개인들은 유동적인 삶을 꾸려가기 위해 전략적 선택을 강요받고 보완했던 것이다. 그런데 그는 벡과 시각을 달리하여 개인들이 공동체를 회복하지 못하고, 되려 전 지구적 위험 앞에서 생존을 위한 자기중심적인 태도와 선택을 할 수밖에 없는 조건에 내몰리고 있다고 진단했다. 바우만은 현대 사회를 ‘소외된 개인들의 사회’로 보면서 책임 회피, 불신, 불안이 소유의 무한 경쟁 속에서 일상화되고, 동시에 시장들이 무한 구매와 소비를 자극함으로써 다양한 생존 자체를 위협받는 조건에 인간은 철저히 무력화되고 있다고 보았다. 그는 인간이 사회적으로 주체성을 잃고 책임을 질 수조차 못하게 된 현대인을 ‘소유에 의해 위기에 내몰린 존재’로 이해하며 벡과는 달리 개체화가 새로운 연대를 도모하지 못하며 자치와 회복의 가능성이 없다고 진단하고 있다. 따라서 바우만은 벡이 제시한 것처럼 정치적 요구를 통해 연대와 공동체를 모색하는 행동은 모두 유동(流動)하게 된다고 본다. 그렇기 때문에 바우만은 일상생활에서의 정치적 요구를 담은 실천 행위도 개체화의 흐름에 놓여 있기 때문에 현대의 위기에 대한 해결책이 될 수 없다고 판단하고 있다."

raw_input_ids = tokenizer.encode(text)
input_ids = [tokenizer.bos_token_id] + raw_input_ids + [tokenizer.eos_token_id]

summary_ids = model.generate(
    torch.tensor([input_ids]),
    max_new_tokens=100,
    num_beams=4,
    repetition_penalty=2.0,
    early_stopping=True
)

summary = tokenizer.decode(summary_ids.squeeze().tolist(), skip_special_tokens=True, clean_up_tokenization_spaces=True)
print("요약 결과:", summary)